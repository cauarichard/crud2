{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Install requirements\n",
        "!git clone https://github.com/InstantID/InstantID\n",
        "%cd InstantID\n",
        "!pip install -r gradio_demo/requirements.txt\n",
        "!pip install timm==0.6.7\n",
        "!pip install diffusers==0.27.2"
      ],
      "metadata": {
        "id": "ZOztpxGQECha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch para compatibilidade do huggingface_hub com diffusers\n",
        "import huggingface_hub\n",
        "\n",
        "if not hasattr(huggingface_hub, \"cached_download\"):\n",
        "    huggingface_hub.cached_download = huggingface_hub.hf_hub_download\n"
      ],
      "metadata": {
        "id": "z7NhDYQrwrdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7PcfK1sXxndl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download required models\n",
        "\n",
        "!python gradio_demo/download_models.py -y"
      ],
      "metadata": {
        "id": "LdoQC6SuFf3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar huggingface_hub para a faixa exigida pelo diffusers\n",
        "!pip install -q --force-reinstall \"huggingface_hub==0.34.0\"\n"
      ],
      "metadata": {
        "id": "84GIu91Q1Sh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"insightface==0.7.3\"\n"
      ],
      "metadata": {
        "id": "JlXvYSXs6e8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências do insightface\n",
        "!pip install -q \"onnxruntime-gpu\" \"onnxruntime\"\n"
      ],
      "metadata": {
        "id": "m_LULSao630E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"controlnet-aux\"\n"
      ],
      "metadata": {
        "id": "mDLrLa3-7XZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMcdj5AzEBo-"
      },
      "outputs": [],
      "source": [
        "# @title Set up the pipeline\n",
        "\n",
        "import diffusers\n",
        "from diffusers.utils import load_image\n",
        "from diffusers.models import ControlNetModel, AutoencoderKL\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from insightface.app import FaceAnalysis\n",
        "from pipeline_stable_diffusion_xl_instantid_img2img import (\n",
        "    StableDiffusionXLInstantIDImg2ImgPipeline,\n",
        "    draw_kps,\n",
        ")\n",
        "from controlnet_aux import ZoeDetector\n",
        "\n",
        "# prepare 'antelopev2' under ./models\n",
        "app = FaceAnalysis(name=\"antelopev2\", root=\"./\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
        "app.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "# prepare models under ./checkpoints\n",
        "face_adapter = \"./checkpoints/ip-adapter.bin\"\n",
        "controlnet_path = \"diffusers/controlnet-zoe-depth-sdxl-1.0\"\n",
        "\n",
        "# load IdentityNet + SDXL público\n",
        "identitynet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
        "zoedepthnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    \"stabilityai/sdxl-vae\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionXLInstantIDImg2ImgPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    controlnet=[identitynet, zoedepthnet],\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
        "    pipe.scheduler.config,\n",
        "    use_karras_sigmas=True,\n",
        ")\n",
        "pipe.to(\"cuda\")\n",
        "pipe.load_ip_adapter_instantid(face_adapter)\n",
        "pipe.set_ip_adapter_scale(0.8)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === GERAÇÃO EM LOTE A PARTIR DO GOOGLE DRIVE (SEM FaceAnalysis) ===\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Montar o Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# 2. Pastas de entrada e saída\n",
        "input_folder = \"/content/drive/MyDrive/dataset/real_fake/faces_real\"\n",
        "output_folder = \"/content/drive/MyDrive/dataset_gerado/instantid_xl\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "print(\"Lendo fotos reais em:\", input_folder)\n",
        "\n",
        "# 3. Lista de imagens reais\n",
        "fotos_reais = sorted(\n",
        "    glob.glob(os.path.join(input_folder, \"*.jpg\")) +\n",
        "    glob.glob(os.path.join(input_folder, \"*.jpeg\")) +\n",
        "    glob.glob(os.path.join(input_folder, \"*.png\"))\n",
        ")\n",
        "\n",
        "print(\"Encontradas\", len(fotos_reais), \"imagens.\")\n",
        "\n",
        "if len(fotos_reais) == 0:\n",
        "    raise SystemExit(f\"Nenhuma imagem encontrada em {input_folder}\")\n",
        "\n",
        "# 4. Estilos (3 variações por rosto)\n",
        "estilos = [\n",
        "    \"Professional face portrait, studio lighting, neutral expression, realistic skin texture\",\n",
        "    \"Close-up photo, happy smiling expression, natural sunlight, outdoor park background\",\n",
        "    \"Cinematic headshot, neon night lighting reflections on face, mysterious style\",\n",
        "]\n",
        "\n",
        "# 5. Loop principal (sem usar FaceAnalysis)\n",
        "for i, caminho in enumerate(fotos_reais, start=1):\n",
        "    nome_id = os.path.splitext(os.path.basename(caminho))[0]\n",
        "    print(f\"\\n[{i}/{len(fotos_reais)}] Processando:\", nome_id)\n",
        "\n",
        "    face_image = Image.open(caminho).convert(\"RGB\")\n",
        "\n",
        "    # usamos a própria imagem como referência/control_image\n",
        "    images = [face_image, face_image]\n",
        "\n",
        "    for v, prompt in enumerate(estilos, start=1):\n",
        "        result = pipe(\n",
        "            prompt=prompt,\n",
        "            image=images,\n",
        "            num_inference_steps=25,\n",
        "            guidance_scale=5.0,\n",
        "        )\n",
        "        img = result.images[0]\n",
        "\n",
        "        out_path = os.path.join(output_folder, f\"{nome_id}_fake_var{v}.png\")\n",
        "        img.save(out_path)\n",
        "        print(f\"   ✓ Variação {v} salva em:\", out_path)\n",
        "\n",
        "print(\"\\n✓ FINALIZADO! Imagens geradas em:\", output_folder)\n"
      ],
      "metadata": {
        "id": "qo-wxg9rNjlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTE: ver se o detector acha rosto em 1 imagem\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "teste_path = \"/content/drive/MyDrive/dataset/real_fake/faces_real/real_id1_0000.mp4_frame_00000.jpg\"  # ajuste se o nome for diferente\n",
        "print(\"Usando imagem de teste:\", teste_path, os.path.exists(teste_path))\n",
        "\n",
        "img = Image.open(teste_path).convert(\"RGB\")\n",
        "img_np = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "infos = app.get(img_np)\n",
        "print(\"Qtde de rostos detectados:\", len(infos))\n",
        "\n",
        "if infos:\n",
        "    print(\"BBox do maior rosto:\", infos[0][\"bbox\"])\n",
        "else:\n",
        "    print(\"Nenhum rosto detectado.\")\n"
      ],
      "metadata": {
        "id": "pa8gJjAO-y-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load your LoRA!\n",
        "# @markdown You can load a LoRA directly from Hugging Face by browsing [here](https://huggingface.co/models?library=diffusers&other=lora), or download a LoRA from CivitAI/Tensor.Art and place it on the colab folder.\n",
        "pipe.load_lora_weights(\n",
        "    \"Norod78/sdxl-chalkboarddrawing-lora\",\n",
        "    weight_name=\"SDXL_ChalkBoardDrawing_LoRA_r8.safetensors\"\n",
        ")\n",
        "pipe.enable_sequential_cpu_offload()"
      ],
      "metadata": {
        "id": "WqOphy2WQY_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate!\n",
        "\n",
        "prompt = \"A colorful ChalkBoardDrawing of a man\" # @param {type:\"string\"}\n",
        "negative_prompt = \"blurry, ultra-realism, detailed\" # @param {type:\"string\"}\n",
        "# @markdown The higher the `denoising_strength`, more similar to the original image.\n",
        "denoising_strength = 0.85 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "guidance_scale = 7 # @param {type:\"number\"}\n",
        "face_control_strength = 0.8 # @param {type:\"slider\", min:0, max:1, step: 0.01}\n",
        "depth_control_strength = 0.8 # @param {type:\"slider\", min:0, max:1, step: 0.01}\n",
        "\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=1024,\n",
        "    height=1024,\n",
        "    image_embeds=face_emb,\n",
        "    image=face_image,\n",
        "    strength=denoising_strength,\n",
        "    control_image=images,\n",
        "    num_inference_steps=20,\n",
        "    guidance_scale = guidance_scale,\n",
        "    controlnet_conditioning_scale=[face_control_strength, depth_control_strength],\n",
        ").images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "ZBenCLFjQgTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}